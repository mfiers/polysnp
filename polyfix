#!/usr/bin/env python

import os
import re
import sys
import argparse
import itertools
import collections
import logging as lg
import subprocess as sp

import numpy as np

import rpy2
import rpy2.robjects as robjects
import rpy2.robjects.numpy2ri
rpy2.robjects.numpy2ri.activate()

from rpy2.robjects.packages import importr
#Rstats = importr('stats')
pvclust = importr('pvclust')
import networkx as nx

import pysam

import vcf

lg.basicConfig(level=lg.INFO)
parser = argparse.ArgumentParser(description='Remove bad calls based on correlation ' +
                                 'with a neighbouring SNPs')

parser.add_argument('vcf', type=str, default='-', nargs='?', 
                    help='vcf file input (determines locations to check, '+
                    '- for stdin)')

parser.add_argument('-v',  help="Verbose output", dest='verbose', 
                    action='store_true', default=False)

parser.add_argument('bam', nargs='+', help='samtools indexed bamfiles to check')
parser.add_argument('-b', dest='base', help='basename for output', default='polyfix')
parser.add_argument('-m', dest='min_allele_count', help='minimal allele count - discard otherwise',
                    type=int, default=2)
parser.add_argument('-M', dest='mask', help='mask read name')
parser.add_argument('-S', dest='stepper', help='stepper to use for pileups (all or samtools)', 
                    default='samtools')
parser.add_argument('-r', type=str, help="region to process",
                    dest='region')


class peekorator(object):
    
    def __init__(self, iterator):
        self.empty = False
        self.peek = None
        self._iterator = iterator
        try:
            self.peek = iterator.next()
        except StopIteration:
            self.empty = True

    def __iter__(self):
        return self

    def next(self):        
        """
        Return the self.peek element, or raise StopIteration 
        if empty
        """
        if self.empty:
            raise StopIteration()
        to_return = self.peek
        try:
            self.peek = self._iterator.next()
        except StopIteration:
            self.peek = None
            self.empty = True
        return to_return


def PileupGenerator(vcf, bamfiles, region=None):
    """
    Return a set of stacked colums from the pileup - one for each sample.        
    """

    plg = lg.getLogger('PiG')
    plg.setLevel(lg.DEBUG)
    pileups = []
    plg.info("opening pileupgenerator with region %s" % region)
    for i, bamfile in enumerate(bamfiles):
        if region:
            pup = peekorator(bamfile.pileup(region=region, stepper=args.stepper))
            pileups.append(pup)
            plg.info("opened pileupgenerator with region %s (%s)" % (region, str(pup)))
        else:
            pup(peekorator(bamfile.pileup(stepper=args.stepper)))
            pileups.append(pup)
            plg.info("opened global pileup generator (%s)" % (str(pup)))
        
    current_ptid = 0
    
    for record in vcf:
        rv = [record]
        #lg.debug("pileup finding record %s" % record)
        for i, pileup in enumerate(pileups):
            sid = nice_sample_names[i]

            #see if we're in range for this pileup?
            if pileup.empty:
                #nothing in here - 
                rv.append(None)
                continue

            #peek at next record
            peek = pileup.peek
            try:
                peekseq = bamfiles[i].getrname(peek.tid)
            except:
                lg.critical("cannot getrname from %s" % peek)
                raise

            assert(peekseq == record.seq) #no seq switching (yet)

            if peek.pos >= record.pos:
                #lg.info('next %d %d' % (peek.pos, record.pos))
                #next pileup position is past the current record position:
                rv.append(None)                
                continue

            #ok - we're good - parse through the seqs until we have somehting
            for p in pileup:
                pseq = bamfiles[i].getrname(p.tid)
                
                #do not have the code for seq switching
                assert(pseq == record.seq)                
                    
                if pseq == record.seq and \
                        p.pos+1 == record.pos:
                    rv.append(p)
                    break            
                if pileup.empty:
                    rv.append(None)
                    break
                peek = pileup.peek
                peekseq = bamfiles[i].getrname(peek.tid)
                if peekseq == record.seq and peek.pos+1 > record.pos:
                    #next position is passed our record
                    rv.append(None)
                    break
            else:
                #it appears we ran out of the pileup - 
                rv.append(None)
            #nextcol = pileup.next()
            #print nextcol
        #lg.debug('yielding %d %s' % (record.pos, str(rv[-1])[:20]))
        #if record.pos == 1880:
        #    for j, r in enumerate(rv[1:]):
        #        print nice_sample_names[j], r
        yield(rv)                          
    
def nid(c,p,n):
    """
    return node id
    """
    return '%s.%s.%s' % (c,p,n)

def eid(a,b):
    """
    edge id
    """
    return ('%s.%s.%s' % a,
            '%s.%s.%s' % b)

def processGraph(alg, rta, sampleid=None):
    """
    Create graph (alg) edges based on the reads_to_alleles (rta) structure 
    """
    
    no_edges = 0
    max_weight = 0
    loci = collections.defaultdict(set)
    linked_loci = set()

    for read in rta:
        for all1 in rta[read]:
            loc1 = (all1[0], all1[1])
            loci[loc1].add(all1)

            #if not alg.has_node(all1): continue
            for all2 in rta[read]:
                loc2 = (all2[0], all2[1])
                loci[loc2].add(all2)

                lnk = tuple(sorted([loc1, loc2]))
                linked_loci.add(lnk)

                if all1 >= all2: continue
                if not alg.has_edge(all1, all2):
                    no_edges += 1
                    alg.add_edge(all1, all2, weight=0)
                alg[all1][all2]['weight'] += 1
                if alg[all1][all2]['weight'] > max_weight:
                    max_weight = alg[all1][all2]['weight']
    lg.info("Edges added: %d" % no_edges)
    lg.info("Max weight: %d" % max_weight)
    lg.info("unique loci %d" % len(loci))

    laar = 0
    for l in loci:
        allcount = sorted([(alg.node[x]['count'], x[2]) for x in loci[l]])
        major_allle = allcount[-1][1]

        #remove all major alleles from the graph 
        alg.remove_node((l[0], l[1], major_allle))
        lg.debug("%s" % str(allcount))
        low_abundant_alleles = [x[1] for x in allcount if x[0] < args.min_allele_count]
        lg.info("removing %d low abundandant alleles" % len(low_abundant_alleles))
        for laa in low_abundant_alleles:
            laar += 1
            if alg.has_node((l[0], l[1], laa)):
                alg.remove_node((l[0], l[1], laa))

    lg.info("removed %d low abundant alleles" % laar)

    #remove zero degree nodes
    to_delete = []
    for n in alg.nodes():
        if alg.degree(n) == 0:
            to_delete.append(n)
    lg.info("removing %d zero degree nodes" % len(to_delete))
    [alg.remove_node(x) for x in to_delete]

    #lg.debug("LAA: %s" % str(low_abundant_alleles))
    
    #now convert to a numpy matrix
    nodes = alg.nodes()
    names = ['%s.%s.%s' % x for x in nodes]
    if args.mask:
        names = [x.replace(args.mask, '') for x in names]

    node2name = dict(zip(nodes, range(len(names))))
    #print node2name
    #create an R matrix - can handle NAs
    m = robjects.r.matrix([0 for x in range(len(nodes)**2)], ncol=len(nodes),nrow=len(nodes))
    m.colnames = robjects.StrVector(names)
    m.rownames = robjects.StrVector(names)
    
    for x in range(len(nodes)):
        for y in range(len(nodes)):
            m.rx[x,y] = robjects.NA_Integer

    #make sure all linked loci are set to zero
    for i in linked_loci:
        for a1 in loci[i[0]]:
            if not a1 in nodes: continue
            for a2 in loci[i[1]]:
                if not a2 in nodes: continue
                m.rx[node2name[a1],node2name[a2]] = 0

    #fill in the real values
    for e in alg.edges():
        w = alg[e[0]][e[1]]['weight']
        m.rx[node2name[e[0]],node2name[e[1]]] = w

    return m
    
if __name__ == '__main__':
    args = parser.parse_args()
    if args.verbose:
        lg.getLogger().setLevel(lg.DEBUG)

    vcf = vcf.PSVCF(args.vcf, region=args.region)
    
    nice_sample_names = vcf.simple_names()
    lg.info("vcf sample names: %s" % ", ".join(nice_sample_names))

    bamfiles = []
    lg.debug('opening bam files')
    for i, bamfile in enumerate(args.bam):
        if not nice_sample_names[i] in bamfile:
            lg.warning("are you sure the order of bamfiles corresponds to the " +
                       "order in the VCF file?")
        lg.debug('opening bam %s' % bamfile)
        bamfiles.append(pysam.Samfile(bamfile, 'rb'))


    
    reads_to_alleles = [collections.defaultdict(list) for x in nice_sample_names]
    
    #allele connection graphs:
    algs = [nx.Graph() for x in nice_sample_names]

    #go through the reads. identify alleles, store what alleles are
    #linked by what reads
    loci_assessed = 0
    alleles_assessed = 0
    alleles_stored = 0
    read_alleles_processed = 0

    for data in PileupGenerator(vcf, bamfiles, args.region):
        record = data[0]
        pileups = data[1:]
        loci_assessed += 1

        assert(len(pileups) == len(bamfiles))
        for i, pileup in enumerate(pileups):
            
            sample = record.samples[i]
            alleles_assessed += 1

            sample_alleles = sample.alleles()

            if len(sample_alleles) < 2:
                #this locus/sample does not need to be processed
                continue

            for a in sample.alleles():
                #lg.debug("adding %s / %s / %s to graph" % (a, nice_sample_names[i], record))
                algs[i].add_node((record.seq, record.pos, a), count=0)

            if pileup == None: continue
            alleles_stored += 1


            # print nice_sample_names[i],  sample
            for read in pileup.pileups:
                nt = read.alignment.seq[read.qpos-1]
                # print read.alignment.qname, nt, read.alignment.is_read1
                read_alleles_processed += 1
                try:
                    algs[i].node[(record.seq, record.pos, nt)]['count'] += 1
                except KeyError:
                    #was not in the set - or so it appears - add it now
                    algs[i].add_node((record.seq, record.pos, nt), count=0)

                reads_to_alleles[i][read.alignment.qname].append(
                    (record.seq, record.pos, nt))
                
    lg.info("No loci assessed: %d" % loci_assessed)
    lg.info("No loci*samples assessed: %d" % alleles_assessed)
    lg.info("No loci*samples stored in graph: %d" % alleles_stored)
    lg.info("No loci*samples*reads processed: %s" % read_alleles_processed)

    #convert the reads-to-alleles structure to a graph linking alleles
    lg.info("samples were %s" % nice_sample_names)

    for i,g in enumerate(algs):
        lg.info("Preparing graph for %s" % nice_sample_names[i])
        rta = reads_to_alleles[i]

        simmat = processGraph(g, rta)
        robjects.r.assign('mx',simmat)
        #robjects.r('dx <- as.dist(sqrt(outer(diag(mx), diag(mx), "+") - 2*mx))')
        robjects.r('dx <- 2*mx')
        dx = robjects.r('dx')
        print dx
        sys.exit()
        # This function returns an object of class "dist"
        #sim2dist <- function(mx) as.dist(sqrt(outer(diag(mx), diag(mx), "+") - 2*mx)) 
        # from similarity to distance matrix 
        #d.mx = as.matrix(d.mx) 
        #d.mx = sim2dist(d.mx) 
        # The distance matrix can be used to visualize 
        # hierarchical clustering results as dendrograms 
        #hc = hclust(d.mx) 
        #plot(hc)


        fit = robjects.r.hclust(simmat, method='average')        
        #fit = pvclust.pvclust(m, method_hclust='ward', method_dist='euclidian')
        #groups = robjects.r.cutree(fit, h=5)
        #print groups
        

        #robjects.r.png('%s.%s.cluster.png' % (args.base, nice_sample_names[i]), 
        #               width=1000, height=1000)
        #robjects.r.plot(fit, labels=nodenames)
        #try:
        #    robjects.r['rect.hclust'](fit, h=4, border='red')
        #except:
        #    pass
        #robjects.r['dev.off']()

